{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644b4eb5-ed8f-455a-9983-a909a170c8d3",
   "metadata": {},
   "source": [
    "While it may seem long, convoluted, and scary, please remember I am learning this too.\n",
    "I was in the same position as you are right now only about a week or two ago.\n",
    "\n",
    "The following code highlights the import of two key modules I plan on using throughout the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ab7800-50a8-4f6d-aaf1-5a4e0f931323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272a260-be78-473c-acf6-c30d4bb45454",
   "metadata": {},
   "source": [
    "(Note that pandas as pd only means I changed the name it is identified as for brevity.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66647587-979f-47c4-a5bf-6700f027f5b8",
   "metadata": {},
   "source": [
    "I'm going to load a few additional items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f58798-d50a-406d-8041-ea7b155da2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Core Libaries\n",
    "nlpen = spacy.load(\"en_core_web_trf\")\n",
    "nlpes = spacy.load(\"es_dep_news_trf\")\n",
    "\n",
    "#Load English/Spanish Data\n",
    "eng = spacy.lang.en.English()\n",
    "esp = spacy.lang.es.Spanish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caee4ab-15d5-4093-8734-5b24def0ba15",
   "metadata": {},
   "source": [
    "The above code is dense to load. The \"Core Libraries\" are trained models in Spanish and English.\n",
    "Essentially, a Trained Language Model is a set of words from a corpus (body of text; blame colonialism for the Latin) that have been through the process of machine learning to be categorized with meaning (part-of-speech, sentiment, similarity, etc.) Machine learning is a statistical process that uses a ton of what are called \"vectors\" (similar to lists [1, 2, 3, 4] but vertical) with bits of data based off of what the machine assumes to be the right assignment for it after being forced to do it based off of previous knowledge over and over again until it is somewhat accurate.\n",
    "\n",
    "ETHICS NOTE: Obviously statistics is flawed, machine learning is flawed, and any language model can and will perpetuate systemic racial biases through language injustice. Colloquialisms or other valid language forms (i.e. AAVE) could potentially be misattributed and tagged falsely as \"incorrect\" due to the racial biases of those training the algorithm and the racial biases of the internet. \n",
    "\n",
    "Now, let's begin actual programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c977ec-bda4-42ad-a3d3-86503cea6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2s(df):\n",
    "    return df.to_string(columns=None, buf=None, index=False, na_rep=\"NaN\")\n",
    "\n",
    "def frequency(txt):\n",
    "    nouns = [token.text for token in txt if (not token.is_stop and not token.is_punct and token.pos_ == \"NOUN\")]\n",
    "    adjectives = [token.text for token in txt if (not token.is_stop and not token.is_punct and token.pos_ == \"ADJ\")]\n",
    "    verbs = [token.text for token in txt if (not token.is_stop and not token.is_punct and token.pos_ == \"VERB\")]\n",
    "    print(\"Most Common Nouns\")\n",
    "    print(Counter(nouns).most_common(20))\n",
    "    print(\"Most Common Adjectives\")\n",
    "    print(Counter(adjectives).most_common(20))\n",
    "    print(\"Most Common Verbs\")\n",
    "    print(Counter(verbs).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4a04a-f116-4ab9-87b6-99f6447baee2",
   "metadata": {},
   "source": [
    "I just created two preprocessing functions that would be useful for our program. d2s() turns the imported data into a large sum of text in a data type called a \"string\". The additional values highlight that I don't want any additional columns, \"buf=None\" allows it to turn into a string, \"index=False\" highlights that I don't want row labels, and \"na_rep=\"NaN\"\" shows what value I want empty cells to take on when converted into the new format. frequency() simply finds all the nouns, adjectives, and verbs in the program, and logs the most frequent 20 of each found in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
